{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from promptsource.templates import DatasetTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/galactica-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 75.5k/75.5k [00:00<00:00, 296kB/s]\n",
      "Generating train split: 100%|██████████| 250/250 [00:00<00:00, 18433.58 examples/s]\n",
      "Generating validation split: 100%|██████████| 56/56 [00:00<00:00, 15053.58 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 26742.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name, subset_name = \"super_glue\", \"cb\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, subset_name, split=\"train\")\n",
    "example = dataset[0]\n",
    "\n",
    "prompts = DatasetTemplates(f\"{dataset_name}/{subset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = DatasetTemplates(\"anli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GPT-3 style',\n",
       " 'MNLI crowdsource',\n",
       " 'always/sometimes/never',\n",
       " 'based on the previous passage',\n",
       " 'can we infer',\n",
       " 'claim true/false/inconclusive',\n",
       " 'consider always/sometimes/never',\n",
       " 'does it follow that',\n",
       " 'does this imply',\n",
       " 'guaranteed true',\n",
       " 'guaranteed/possible/impossible',\n",
       " 'justified in saying',\n",
       " 'must be true',\n",
       " 'should assume',\n",
       " 'take the following as truth']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = prompts[\"does it follow that\"]\n",
    "# prompt.apply(example)\n",
    "prompts.all_template_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 777k/777k [00:00<00:00, 1.74MB/s]\n",
      "Downloading data: 100%|██████████| 781k/781k [00:01<00:00, 481kB/s]t]\n",
      "Downloading data: 100%|██████████| 35.3M/35.3M [00:07<00:00, 4.93MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:13<00:00,  4.36s/it]\n",
      "Generating train split: 45460 examples [00:12, 3727.22 examples/s]\n",
      "Generating validation split: 1000 examples [00:00, 3819.50 examples/s]\n",
      "Generating test split: 1000 examples [00:00, 3759.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer_choices', 'inputs', 'inputs_pretokenized', 'targets', 'targets_pretokenized'],\n",
       "        num_rows: 45460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['answer_choices', 'inputs', 'inputs_pretokenized', 'targets', 'targets_pretokenized'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['answer_choices', 'inputs', 'inputs_pretokenized', 'targets', 'targets_pretokenized'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly pick a instruction template, followin the settings in Section 4.1 of the paper -- 0.5% dataset\n",
    "p3_subsets = [\n",
    "    'super_glue_rte_does_it_follow_that', # RTE\n",
    "    'super_glue_cb_does_it_follow_that', # CB\n",
    "    'anli_does_it_follow_that_r1', # ANLI R1\n",
    "    'anli_does_it_follow_that_r2', # ANLI R2\n",
    "    'anli_does_it_follow_that_r3', # ANLI R3\n",
    "    'super_glue_copa_cause_effect', # COPA\n",
    "    'hellaswag_complete_first_then', # HelloSwag\n",
    "    'winogrande_winogrande_xl_fill_in_the_blank', # Winogrande\n",
    "    'super_glue_wsc.fixed_replaced_with', # WSC\n",
    "    'super_glue_wic_question_context', # WIC\n",
    "]\n",
    "dataset = load_dataset(\"bigscience/P3\", p3_subsets[3])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
